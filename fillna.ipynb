{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7820/4092056944.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fill_nan[c] = le.transform(fill_nan[c])\n",
      "/tmp/ipykernel_7820/4092056944.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fill_nan2[c] = le.transform(fill_nan2[c])\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/\"\n",
    "book = pd.read_csv(path+\"books.csv\")\n",
    "users = pd.read_csv(path+\"users.csv\")\n",
    "train_ratings = pd.read_csv(path+\"train_ratings.csv\")\n",
    "test_ratings = pd.read_csv(path+\"test_ratings.csv\")\n",
    "data = pd.merge(pd.merge(book, train_ratings, on=\"isbn\"), users, on=\"user_id\")\n",
    "test_data = pd.merge(pd.merge(book, test_ratings, on=\"isbn\"), users, on=\"user_id\")\n",
    "\n",
    "data[\"location_city\"] = data[\"location\"].apply(lambda x : x.split(',')[0])\n",
    "data[\"location_state\"] = data[\"location\"].apply(lambda x : x.split(',')[1])\n",
    "data[\"location_country\"] =  data[\"location\"].apply(lambda x : x.split(',')[2])\n",
    "\n",
    "test_data[\"location_city\"] = test_data[\"location\"].apply(lambda x : x.split(',')[0])\n",
    "test_data[\"location_state\"] = test_data[\"location\"].apply(lambda x : x.split(',')[1])\n",
    "test_data[\"location_country\"] =  test_data[\"location\"].apply(lambda x : x.split(',')[2])\n",
    "\n",
    "column = 'category'\n",
    "data.loc[~data[column].isna(), column] = data.loc[~data[column].isna(), column].apply(lambda x : re.sub(\"[\\'\\[\\]]\",\"\",x).lower())\n",
    "test_data.loc[~test_data[column].isna(), column] = test_data.loc[~test_data[column].isna(), column].apply(lambda x : re.sub(\"[\\'\\[\\]]\",\"\",x).lower())\n",
    "# data[column].describe().to_frame()\n",
    "\n",
    "columns = [\"book_title\", \"year_of_publication\", \"publisher\", \"language\", \"category\", \"user_id\", \"age\", \"location_city\", \"location_state\", \"location_country\"]\n",
    "fill_nan = data[columns]\n",
    "fill_nan2 = test_data[columns]\n",
    "train_test_ = pd.concat([fill_nan, fill_nan2], axis=0)\n",
    "for c in fill_nan.columns:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_test_[c])\n",
    "    fill_nan[c] = le.transform(fill_nan[c])\n",
    "    fill_nan2[c] = le.transform(fill_nan2[c])\n",
    "train_test_ = pd.concat([fill_nan, fill_nan2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install deepimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ = train_test_.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all the cores (8)\n",
      "Input dataset is 383494 cells (rows) and 9 genes (columns)\n",
      "First 3 rows and columns:\n",
      "   book_title  year_of_publication  publisher\n",
      "0       20344                   89       4598\n",
      "1      131187                   90       9212\n",
      "2      111089                   92       4937\n",
      "512 genes selected for imputation\n",
      "Net 0: 0 predictors, 512 targets\n",
      "Normalization\n",
      "Building network\n",
      "[{'type': 'dense', 'neurons': 256, 'activation': 'relu'}, {'type': 'dropout', 'rate': 0.2}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with 383494 cells\n",
      "Epoch 1/10\n",
      "2847/2847 [==============================] - 11s 4ms/step - loss: 340.2342 - val_loss: 231.7168\n",
      "Epoch 2/10\n",
      "2847/2847 [==============================] - 10s 4ms/step - loss: 152.1802 - val_loss: 88.5197\n",
      "Epoch 3/10\n",
      "2847/2847 [==============================] - 11s 4ms/step - loss: 50.9083 - val_loss: 23.7033\n",
      "Epoch 4/10\n",
      "2847/2847 [==============================] - 11s 4ms/step - loss: 12.2770 - val_loss: 5.8607\n",
      "Epoch 5/10\n",
      "2847/2847 [==============================] - 11s 4ms/step - loss: 4.6901 - val_loss: 4.3592\n",
      "Epoch 6/10\n",
      "2847/2847 [==============================] - 11s 4ms/step - loss: 4.3490 - val_loss: 4.3580\n",
      "Epoch 7/10\n",
      "2847/2847 [==============================] - 11s 4ms/step - loss: 4.3489 - val_loss: 4.3579\n",
      "Epoch 8/10\n",
      "2847/2847 [==============================] - 11s 4ms/step - loss: 4.3489 - val_loss: 4.3578\n",
      "Epoch 9/10\n",
      "2847/2847 [==============================] - 11s 4ms/step - loss: 4.3489 - val_loss: 4.3580\n",
      "Epoch 10/10\n",
      "2847/2847 [==============================] - 12s 4ms/step - loss: 4.3490 - val_loss: 4.3579\n",
      "Stopped fitting after 10 epochs\n",
      "Saved model to disk\n",
      "600/600 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepimpute.multinet.MultiNet at 0x7fee56eb3af0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import ceil\n",
    "from deepimpute.multinet import MultiNet\n",
    "NN_params = {\n",
    "        # 'loss': \"CategoricalCrossentropy\",\n",
    "        'learning_rate': 1e-3,\n",
    "        'batch_size': 128,\n",
    "        'max_epochs': 10,\n",
    "        # 'ncores': 5,\n",
    "        # 'sub_outputdim': 512,\n",
    "        # 'architecture': [\n",
    "            # {\"type\": \"dense\", \"activation\": \"relu\", \"neurons\": 200},\n",
    "            # {\"type\": \"dropout\", \"activation\": \"dropout\", \"rate\": 0.3},]\n",
    "    }\n",
    "model = MultiNet(**NN_params)\n",
    "model.fit(train_test_.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2397/2397 [==============================] - 4s 2ms/step\n",
      "Filling zeros\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_title</th>\n",
       "      <th>year_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>language</th>\n",
       "      <th>category</th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_state</th>\n",
       "      <th>location_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20344.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>4598.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>2719.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>7297.0</td>\n",
       "      <td>913.0</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76544.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>820.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4259.0</td>\n",
       "      <td>2719.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>7297.0</td>\n",
       "      <td>913.0</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12356.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>8073.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>2719.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>7297.0</td>\n",
       "      <td>913.0</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75957.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>10204.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2842.0</td>\n",
       "      <td>2719.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>7297.0</td>\n",
       "      <td>913.0</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87161.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>983.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2759.0</td>\n",
       "      <td>2719.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>7297.0</td>\n",
       "      <td>913.0</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76694</th>\n",
       "      <td>28537.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>11189.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4259.0</td>\n",
       "      <td>67697.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>7203.0</td>\n",
       "      <td>913.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76695</th>\n",
       "      <td>117178.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>7854.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4259.0</td>\n",
       "      <td>67716.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>11645.0</td>\n",
       "      <td>913.0</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76696</th>\n",
       "      <td>59089.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>5740.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4259.0</td>\n",
       "      <td>67745.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4166.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76697</th>\n",
       "      <td>95831.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3932.0</td>\n",
       "      <td>67921.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>7487.0</td>\n",
       "      <td>784.0</td>\n",
       "      <td>322.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76698</th>\n",
       "      <td>74680.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>6917.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2332.0</td>\n",
       "      <td>67993.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>11195.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>322.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76699 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       book_title  year_of_publication  publisher  language  category  \\\n",
       "0         20344.0                 89.0     4598.0       4.0     144.0   \n",
       "1         76544.0                 81.0      820.0      26.0    4259.0   \n",
       "2         12356.0                 82.0     8073.0       4.0    2398.0   \n",
       "3         75957.0                 87.0    10204.0       4.0    2842.0   \n",
       "4         87161.0                 90.0      983.0       4.0    2759.0   \n",
       "...           ...                  ...        ...       ...       ...   \n",
       "76694     28537.0                 82.0    11189.0      26.0    4259.0   \n",
       "76695    117178.0                 72.0     7854.0      26.0    4259.0   \n",
       "76696     59089.0                 78.0     5740.0       8.0    4259.0   \n",
       "76697     95831.0                 82.0     1065.0       4.0    3932.0   \n",
       "76698     74680.0                 87.0     6917.0       4.0    2332.0   \n",
       "\n",
       "       user_id   age  location_city  location_state  location_country  \n",
       "0       2719.0  91.0         7297.0           913.0             193.0  \n",
       "1       2719.0  91.0         7297.0           913.0             193.0  \n",
       "2       2719.0  91.0         7297.0           913.0             193.0  \n",
       "3       2719.0  91.0         7297.0           913.0             193.0  \n",
       "4       2719.0  91.0         7297.0           913.0             193.0  \n",
       "...        ...   ...            ...             ...               ...  \n",
       "76694  67697.0  91.0         7203.0           913.0             111.0  \n",
       "76695  67716.0  91.0        11645.0           913.0             206.0  \n",
       "76696  67745.0  30.0         4166.0            71.0             105.0  \n",
       "76697  67921.0  36.0         7487.0           784.0             322.0  \n",
       "76698  67993.0  34.0        11195.0           254.0             322.0  \n",
       "\n",
       "[76699 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_nan = model.predict(fill_nan)\n",
    "fill_nan2 = model.predict(fill_nan2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_title</th>\n",
       "      <th>year_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>language</th>\n",
       "      <th>category</th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_state</th>\n",
       "      <th>location_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20344</td>\n",
       "      <td>89</td>\n",
       "      <td>4598</td>\n",
       "      <td>4</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>10869</td>\n",
       "      <td>1055</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>131187</td>\n",
       "      <td>90</td>\n",
       "      <td>9212</td>\n",
       "      <td>4</td>\n",
       "      <td>2398</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>10869</td>\n",
       "      <td>1055</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111089</td>\n",
       "      <td>92</td>\n",
       "      <td>4937</td>\n",
       "      <td>26</td>\n",
       "      <td>4259</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>10869</td>\n",
       "      <td>1055</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53653</td>\n",
       "      <td>87</td>\n",
       "      <td>6770</td>\n",
       "      <td>26</td>\n",
       "      <td>4259</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>10869</td>\n",
       "      <td>1055</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120495</td>\n",
       "      <td>86</td>\n",
       "      <td>1595</td>\n",
       "      <td>26</td>\n",
       "      <td>4259</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>10869</td>\n",
       "      <td>1055</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306790</th>\n",
       "      <td>96959</td>\n",
       "      <td>88</td>\n",
       "      <td>10985</td>\n",
       "      <td>26</td>\n",
       "      <td>4259</td>\n",
       "      <td>67957</td>\n",
       "      <td>49</td>\n",
       "      <td>10483</td>\n",
       "      <td>1381</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306791</th>\n",
       "      <td>42161</td>\n",
       "      <td>87</td>\n",
       "      <td>9552</td>\n",
       "      <td>4</td>\n",
       "      <td>689</td>\n",
       "      <td>68012</td>\n",
       "      <td>69</td>\n",
       "      <td>11306</td>\n",
       "      <td>397</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306792</th>\n",
       "      <td>116946</td>\n",
       "      <td>90</td>\n",
       "      <td>8549</td>\n",
       "      <td>26</td>\n",
       "      <td>4259</td>\n",
       "      <td>68014</td>\n",
       "      <td>91</td>\n",
       "      <td>5059</td>\n",
       "      <td>50</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306793</th>\n",
       "      <td>124396</td>\n",
       "      <td>69</td>\n",
       "      <td>983</td>\n",
       "      <td>26</td>\n",
       "      <td>4259</td>\n",
       "      <td>68022</td>\n",
       "      <td>28</td>\n",
       "      <td>11227</td>\n",
       "      <td>1511</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306794</th>\n",
       "      <td>75800</td>\n",
       "      <td>73</td>\n",
       "      <td>7854</td>\n",
       "      <td>4</td>\n",
       "      <td>943</td>\n",
       "      <td>68031</td>\n",
       "      <td>58</td>\n",
       "      <td>206</td>\n",
       "      <td>947</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306795 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        book_title  year_of_publication  publisher  language  category  \\\n",
       "0            20344                   89       4598         4       144   \n",
       "1           131187                   90       9212         4      2398   \n",
       "2           111089                   92       4937        26      4259   \n",
       "3            53653                   87       6770        26      4259   \n",
       "4           120495                   86       1595        26      4259   \n",
       "...            ...                  ...        ...       ...       ...   \n",
       "306790       96959                   88      10985        26      4259   \n",
       "306791       42161                   87       9552         4       689   \n",
       "306792      116946                   90       8549        26      4259   \n",
       "306793      124396                   69        983        26      4259   \n",
       "306794       75800                   73       7854         4       943   \n",
       "\n",
       "        user_id  age  location_city  location_state  location_country  \n",
       "0             0   91          10869            1055                56  \n",
       "1             0   91          10869            1055                56  \n",
       "2             0   91          10869            1055                56  \n",
       "3             0   91          10869            1055                56  \n",
       "4             0   91          10869            1055                56  \n",
       "...         ...  ...            ...             ...               ...  \n",
       "306790    67957   49          10483            1381               322  \n",
       "306791    68012   69          11306             397                56  \n",
       "306792    68014   91           5059              50               322  \n",
       "306793    68022   28          11227            1511               322  \n",
       "306794    68031   58            206             947               322  \n",
       "\n",
       "[306795 rows x 10 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:12:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"verbose\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# reg = CatBoostRegressor(iterations=10000, learning_rate=1e-1, random_seed=777, task_type=\"GPU\")\n",
    "# reg.fit(new_train.iloc[:,:-1], new_train.iloc[:, -1])\n",
    "# pred = reg.predict(new_test)\n",
    "\n",
    "xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.1, max_depth=10, verbose=1, random_state=123)\n",
    "xgb_model.fit(fill_nan, data['rating'])\n",
    "pred = xgb_model.predict(fill_nan2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(\"/opt/ml/input/code/jmkim/level1_bookratingprediction_recsys-level-recsys-07/data/sample_submission.csv\")\n",
    "submit.rating = pred\n",
    "submit.to_csv(\"MultiNet-XGBREG.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import re\n",
    "# import matplotlib.pyplot as plt\n",
    "# # import seaborn as sns\n",
    "# import scipy.stats as spst\n",
    "# import sklearn\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# path = \"./data/\"\n",
    "# book = pd.read_csv(path+\"books.csv\")\n",
    "# users = pd.read_csv(path+\"users.csv\")\n",
    "# train_ratings = pd.read_csv(path+\"train_ratings.csv\")\n",
    "# test_ratings = pd.read_csv(path+\"test_ratings.csv\")\n",
    "# data = pd.merge(pd.merge(book, train_ratings, on=\"isbn\"), users, on=\"user_id\")\n",
    "# test_data = pd.merge(pd.merge(book, test_ratings, on=\"isbn\"), users, on=\"user_id\")\n",
    "\n",
    "# data[\"location_city\"] = data[\"location\"].apply(lambda x : x.split(',')[0])\n",
    "# data[\"location_state\"] = data[\"location\"].apply(lambda x : x.split(',')[1])\n",
    "# data[\"location_country\"] =  data[\"location\"].apply(lambda x : x.split(',')[2])\n",
    "\n",
    "# test_data[\"location_city\"] = test_data[\"location\"].apply(lambda x : x.split(',')[0])\n",
    "# test_data[\"location_state\"] = test_data[\"location\"].apply(lambda x : x.split(',')[1])\n",
    "# test_data[\"location_country\"] =  test_data[\"location\"].apply(lambda x : x.split(',')[2])\n",
    "\n",
    "# column = 'category'\n",
    "# data.loc[~data[column].isna(), column] = data.loc[~data[column].isna(), column].apply(lambda x : re.sub(\"[\\'\\[\\]]\",\"\",x).lower())\n",
    "# test_data.loc[~test_data[column].isna(), column] = test_data.loc[~test_data[column].isna(), column].apply(lambda x : re.sub(\"[\\'\\[\\]]\",\"\",x).lower())\n",
    "# # data[column].describe().to_frame()\n",
    "\n",
    "# columns = [\"book_title\", \"year_of_publication\", \"publisher\", \"language\", \"category\", \"user_id\", \"age\", \"location_city\", \"location_state\", \"location_country\"]\n",
    "# fill_nan = data[columns]\n",
    "# fill_nan2 = test_data[columns]\n",
    "# train_test_ = pd.concat([fill_nan, fill_nan2], axis=0)\n",
    "# for c in fill_nan.columns:\n",
    "#     le = LabelEncoder()\n",
    "#     le.fit(train_test_[c])\n",
    "#     fill_nan[c] = le.transform(fill_nan[c])\n",
    "#     fill_nan2[c] = le.transform(fill_nan2[c])\n",
    "# imp = KNNImputer()\n",
    "# fill_nan = imp.fit_transform(fill_nan)\n",
    "# fill_nan2 = imp.transform(fill_nan2)\n",
    "# # imp = IterativeImputer(\n",
    "# #     estimator=[(CatBoostClassifier(verbose=False), slice(1, 10))],\n",
    "# #     transformers=[(OneHotEncoder(sparse=False), slice(1, 10))],\n",
    "# #     initial_strategy=\"most_frequent\",\n",
    "# #     verbose=1\n",
    "# # )\n",
    "# # X_filled = imp.fit_transform(fill_nan)\n",
    "# # print(X_filled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
