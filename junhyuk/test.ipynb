{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocess_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='./data/'\n",
    "\n",
    "users = pd.read_csv(path + 'users_preprocessed.csv')\n",
    "books = pd.read_csv(path + 'books_merged.csv')\n",
    "train = pd.read_csv(path + 'train_ratings.csv')\n",
    "test = pd.read_csv(path + 'test_ratings.csv')\n",
    "sub = pd.read_csv(path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68092 entries, 0 to 68091\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   user_id           68092 non-null  int64  \n",
      " 1   age               67931 non-null  float64\n",
      " 2   location_city     68079 non-null  object \n",
      " 3   location_state    67950 non-null  object \n",
      " 4   location_country  67920 non-null  object \n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 306795 entries, 0 to 306794\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   user_id  306795 non-null  int64 \n",
      " 1   isbn     306795 non-null  object\n",
      " 2   rating   306795 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 7.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 149570 entries, 0 to 149569\n",
      "Data columns (total 18 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   isbn                 149570 non-null  object \n",
      " 1   book_title           149570 non-null  object \n",
      " 2   year_of_publication  149570 non-null  float64\n",
      " 3   publisher            149570 non-null  object \n",
      " 4   img_url              149570 non-null  object \n",
      " 5   language             82343 non-null   object \n",
      " 6   summary              82343 non-null   object \n",
      " 7   img_path             149570 non-null  object \n",
      " 8   category_high        80719 non-null   object \n",
      " 9   book_author          149570 non-null  object \n",
      " 10  category             80109 non-null   object \n",
      " 11  new_language         148470 non-null  object \n",
      " 12  remove_country_code  148470 non-null  object \n",
      " 13  book_author_over3    149570 non-null  object \n",
      " 14  book_author_over5    149570 non-null  object \n",
      " 15  book_author_over10   149570 non-null  object \n",
      " 16  book_author_over50   149570 non-null  object \n",
      " 17  book_author_over100  149570 non-null  object \n",
      "dtypes: float64(1), object(17)\n",
      "memory usage: 20.5+ MB\n"
     ]
    }
   ],
   "source": [
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_name = ['isbn', 'category_high', 'publisher', 'new_language', 'book_author_over10']\n",
    "context_df = train.merge(users, on='user_id', how='left').merge(books[features_name], on='isbn', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "      <th>age</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_state</th>\n",
       "      <th>location_country</th>\n",
       "      <th>category_high</th>\n",
       "      <th>publisher</th>\n",
       "      <th>new_language</th>\n",
       "      <th>book_author_over10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>ontario_others</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>actresses</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67544</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>7</td>\n",
       "      <td>30.0</td>\n",
       "      <td>toronto</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>actresses</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123629</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>8</td>\n",
       "      <td>30.2</td>\n",
       "      <td>kingston</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>actresses</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200273</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>8</td>\n",
       "      <td>18.0</td>\n",
       "      <td>ontario_others</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>actresses</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>210926</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>9</td>\n",
       "      <td>26.5</td>\n",
       "      <td>guelph</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>actresses</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306790</th>\n",
       "      <td>278843</td>\n",
       "      <td>0743525493</td>\n",
       "      <td>7</td>\n",
       "      <td>28.0</td>\n",
       "      <td>california_others</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simon &amp; Schuster Audio</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306791</th>\n",
       "      <td>278851</td>\n",
       "      <td>067161746X</td>\n",
       "      <td>6</td>\n",
       "      <td>33.0</td>\n",
       "      <td>dallas</td>\n",
       "      <td>texas</td>\n",
       "      <td>usa</td>\n",
       "      <td>humor</td>\n",
       "      <td>Pocket Books</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306792</th>\n",
       "      <td>278851</td>\n",
       "      <td>0884159221</td>\n",
       "      <td>7</td>\n",
       "      <td>33.0</td>\n",
       "      <td>dallas</td>\n",
       "      <td>texas</td>\n",
       "      <td>usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lone Star Books</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306793</th>\n",
       "      <td>278851</td>\n",
       "      <td>0912333022</td>\n",
       "      <td>7</td>\n",
       "      <td>33.0</td>\n",
       "      <td>dallas</td>\n",
       "      <td>texas</td>\n",
       "      <td>usa</td>\n",
       "      <td>fiction</td>\n",
       "      <td>Kqed Books</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306794</th>\n",
       "      <td>278851</td>\n",
       "      <td>1569661057</td>\n",
       "      <td>10</td>\n",
       "      <td>33.0</td>\n",
       "      <td>dallas</td>\n",
       "      <td>texas</td>\n",
       "      <td>usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American Map Corporation</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306795 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id        isbn  rating   age      location_city location_state  \\\n",
       "0             8  0002005018       4  18.0     ontario_others        ontario   \n",
       "1         67544  0002005018       7  30.0            toronto        ontario   \n",
       "2        123629  0002005018       8  30.2           kingston        ontario   \n",
       "3        200273  0002005018       8  18.0     ontario_others        ontario   \n",
       "4        210926  0002005018       9  26.5             guelph        ontario   \n",
       "...         ...         ...     ...   ...                ...            ...   \n",
       "306790   278843  0743525493       7  28.0  california_others     california   \n",
       "306791   278851  067161746X       6  33.0             dallas          texas   \n",
       "306792   278851  0884159221       7  33.0             dallas          texas   \n",
       "306793   278851  0912333022       7  33.0             dallas          texas   \n",
       "306794   278851  1569661057      10  33.0             dallas          texas   \n",
       "\n",
       "       location_country category_high                 publisher new_language  \\\n",
       "0                canada     actresses     HarperFlamingo Canada           en   \n",
       "1                canada     actresses     HarperFlamingo Canada           en   \n",
       "2                canada     actresses     HarperFlamingo Canada           en   \n",
       "3                canada     actresses     HarperFlamingo Canada           en   \n",
       "4                canada     actresses     HarperFlamingo Canada           en   \n",
       "...                 ...           ...                       ...          ...   \n",
       "306790              usa           NaN    Simon & Schuster Audio           en   \n",
       "306791              usa         humor              Pocket Books           en   \n",
       "306792              usa           NaN           Lone Star Books           en   \n",
       "306793              usa       fiction                Kqed Books           en   \n",
       "306794              usa           NaN  American Map Corporation           en   \n",
       "\n",
       "       book_author_over10  \n",
       "0                  others  \n",
       "1                  others  \n",
       "2                  others  \n",
       "3                  others  \n",
       "4                  others  \n",
       "...                   ...  \n",
       "306790             others  \n",
       "306791             others  \n",
       "306792             others  \n",
       "306793             others  \n",
       "306794             others  \n",
       "\n",
       "[306795 rows x 11 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "users = preprocess_location(preprocess_age(users))\n",
    "ratings = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "print(type(features_name))\n",
    "\n",
    "context_df = ratings.merge(users, on='user_id', how='left').merge(books[features_name], on='isbn', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "      <th>age</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_state</th>\n",
       "      <th>location_country</th>\n",
       "      <th>category_high</th>\n",
       "      <th>publisher</th>\n",
       "      <th>new_language</th>\n",
       "      <th>book_author_over10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>4</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>ontario_others</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>actresses</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67544</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>7</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>toronto</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>actresses</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123629</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>8</td>\n",
       "      <td>30.200000</td>\n",
       "      <td>kingston</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>actresses</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200273</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>8</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>ontario_others</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>actresses</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>210926</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>9</td>\n",
       "      <td>26.500000</td>\n",
       "      <td>guelph</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>actresses</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383489</th>\n",
       "      <td>278543</td>\n",
       "      <td>1576734218</td>\n",
       "      <td>0</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>california_others</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "      <td>family</td>\n",
       "      <td>Multnomah</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383490</th>\n",
       "      <td>278563</td>\n",
       "      <td>3492223710</td>\n",
       "      <td>0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>wien</td>\n",
       "      <td>wien</td>\n",
       "      <td>austria</td>\n",
       "      <td>others</td>\n",
       "      <td>Piper</td>\n",
       "      <td>de</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383491</th>\n",
       "      <td>278633</td>\n",
       "      <td>1896095186</td>\n",
       "      <td>0</td>\n",
       "      <td>31.833333</td>\n",
       "      <td>sandy</td>\n",
       "      <td>utah</td>\n",
       "      <td>usa</td>\n",
       "      <td>fiction</td>\n",
       "      <td>Polestar Book Publishers</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383492</th>\n",
       "      <td>278668</td>\n",
       "      <td>8408044079</td>\n",
       "      <td>0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>madrid</td>\n",
       "      <td>madrid</td>\n",
       "      <td>spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Planeta Publishing Corporation</td>\n",
       "      <td>es</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383493</th>\n",
       "      <td>278851</td>\n",
       "      <td>0767907566</td>\n",
       "      <td>0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>dallas</td>\n",
       "      <td>texas</td>\n",
       "      <td>usa</td>\n",
       "      <td>nature</td>\n",
       "      <td>Broadway Books</td>\n",
       "      <td>en</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>383494 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id        isbn  rating        age      location_city  \\\n",
       "0             8  0002005018       4  18.000000     ontario_others   \n",
       "1         67544  0002005018       7  30.000000            toronto   \n",
       "2        123629  0002005018       8  30.200000           kingston   \n",
       "3        200273  0002005018       8  18.000000     ontario_others   \n",
       "4        210926  0002005018       9  26.500000             guelph   \n",
       "...         ...         ...     ...        ...                ...   \n",
       "383489   278543  1576734218       0  39.000000  california_others   \n",
       "383490   278563  3492223710       0  37.000000               wien   \n",
       "383491   278633  1896095186       0  31.833333              sandy   \n",
       "383492   278668  8408044079       0  48.000000             madrid   \n",
       "383493   278851  0767907566       0  33.000000             dallas   \n",
       "\n",
       "       location_state location_country category_high  \\\n",
       "0             ontario           canada     actresses   \n",
       "1             ontario           canada     actresses   \n",
       "2             ontario           canada     actresses   \n",
       "3             ontario           canada     actresses   \n",
       "4             ontario           canada     actresses   \n",
       "...               ...              ...           ...   \n",
       "383489     california              usa        family   \n",
       "383490           wien          austria        others   \n",
       "383491           utah              usa       fiction   \n",
       "383492         madrid            spain           NaN   \n",
       "383493          texas              usa        nature   \n",
       "\n",
       "                             publisher new_language book_author_over10  \n",
       "0                HarperFlamingo Canada           en             others  \n",
       "1                HarperFlamingo Canada           en             others  \n",
       "2                HarperFlamingo Canada           en             others  \n",
       "3                HarperFlamingo Canada           en             others  \n",
       "4                HarperFlamingo Canada           en             others  \n",
       "...                                ...          ...                ...  \n",
       "383489                       Multnomah           en             others  \n",
       "383490                           Piper           de             others  \n",
       "383491        Polestar Book Publishers           en             others  \n",
       "383492  Planeta Publishing Corporation           es             others  \n",
       "383493                  Broadway Books           en             others  \n",
       "\n",
       "[383494 rows x 11 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_context_data(users, books, ratings1, ratings2, features_name:list):\n",
    "    # publisher 전처리\n",
    "    # books = preprocess_publisher(books)\n",
    "    # age, location 전처리\n",
    "    users = preprocess_location(preprocess_age(users))\n",
    "    ratings = pd.concat([ratings1, ratings2]).reset_index(drop=True)\n",
    "\n",
    "    print(type(features_name))\n",
    "    \n",
    "    # 인덱싱 처리된 데이터 조인\n",
    "    \"\"\"\n",
    "    users_preprocessed:\n",
    "        dataframe\n",
    "        user_id,age,location_city,location_state,location_country\n",
    "        \n",
    "    books_merged:\n",
    "        dataframe\n",
    "        isbn,book_title,year_of_publication,publisher,img_url,\n",
    "        language,summary,img_path,category_high,book_author,category,\n",
    "        new_language,remove_country_code,book_author_over3,book_author_over5,\n",
    "        book_author_over10,book_author_over50,book_author_over100\n",
    "    \"\"\"\n",
    "    # user_id, isbn, age, city, state, country, category_high, publisher_4_digit, language, author_10\n",
    "    # context_df = ratings.merge(users, on='user_id', how='left').merge(books[features_name], on='isbn', how='left')\n",
    "    context_df = ratings.merge(users, on='user_id', how='left').merge(books[features_name], on='isbn', how='left')\n",
    "    train_df = ratings1.merge(users, on='user_id', how='left').merge(books[features_name], on='isbn', how='left')\n",
    "    test_df = ratings2.merge(users, on='user_id', how='left').merge(books[features_name], on='isbn', how='left')\n",
    "\n",
    "    idx = dict()\n",
    "    # users 인덱싱\n",
    "    idx, train_df, test_df = users2idx(context_df, train_df, test_df, idx)\n",
    "    \n",
    "    # books 인덱싱\n",
    "    idx, train_df, test_df = books2idx(context_df, train_df, test_df, features_name, idx)\n",
    "    \n",
    "    return idx, train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCollaborativeFiltering:\n",
    "    def __init__(self, args, data):\n",
    "        super().__init__()\n",
    "\n",
    "        self.criterion = RMSELoss()\n",
    "        self.train_dataloader = data['train_dataloader']\n",
    "        self.valid_dataloader = data['valid_dataloader']\n",
    "        self.field_dims = data['field_dims']\n",
    "        \n",
    "        self.field_idx_dict = data['field_name_dim_dict']\n",
    "        \n",
    "        self.embed_dim = args.NCF_EMBED_DIM\n",
    "        self.epochs = args.EPOCHS\n",
    "        self.learning_rate = args.LR\n",
    "        self.weight_decay = args.WEIGHT_DECAY\n",
    "        self.log_interval = 100\n",
    "\n",
    "        self.device = args.DEVICE\n",
    "\n",
    "        self.mlp_dims = args.NCF_MLP_DIMS\n",
    "        self.dropout = args.NCF_DROPOUT\n",
    "        self.batch_size = args.BATCH_SIZE\n",
    "        \n",
    "        self.model = _NeuralCollaborativeFiltering(self.field_dims, field_idx_dict=self.field_idx_dict, embed_dim=self.embed_dim,\n",
    "                                                    mlp_dims=self.mlp_dims, dropout=self.dropout, batch_size=self.batch_size).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(params=self.model.parameters(), lr=self.learning_rate, amsgrad=True, weight_decay=self.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _NeuralCollaborativeFiltering(nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims, field_idx_dict, embed_dim, mlp_dims, dropout, batch_size):\n",
    "        super().__init__()\n",
    "        self.field_idx_dict = field_idx_dict\n",
    "        \"\"\"\n",
    "        FeaturesEmbedding(field_dims, embed_dim)\n",
    "            field_dims: [유저 전체 수, 아이템 전체 수] == np.array([len(user2idx), len(isbn2idx)], dtype=np.uint32)\n",
    "            embed_dim: args.NCF_EMBED_DIM -> user_vector의 가로 길이\n",
    "                몇 k차원으로 임베딩할 것인가\n",
    "            self.embedding 통과 후: 2 * embed_dim으로 변환\n",
    "            [ 68069 149570], 16\n",
    "            68069 149570 -> 20만\n",
    "        \"\"\"\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.embed_output_dim = len(field_dims) * embed_dim\n",
    "        self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropout, output_layer=False)\n",
    "        self.fc = torch.nn.Linear(mlp_dims[-1] + embed_dim, 1)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_user_fields)``\n",
    "        \"\"\"\n",
    "        \n",
    "        # print(x.shape) torch.Size([1024, (2 + context_feature 수)])\n",
    "        # user & item vector -> 임베딩\n",
    "        x = self.embedding(x)\n",
    "        # print(x.shape) torch.Size([1024, (2 + context_feature 수), 16])\n",
    "        \n",
    "        # user_x = x[:, self.user_field_idx].squeeze(1)\n",
    "        # item_x = x[:, self.item_field_idx].squeeze(1)\n",
    "        # gmf = user_x * item_x\n",
    "        \n",
    "        gmf = x[:, self.field_idx_dict['user_id']].squeeze(1)\n",
    "        for field_name, field_idx in self.field_idx_dict.items():\n",
    "            if field_name == 'user_id':\n",
    "                continue\n",
    "            tmp = x[:, self.field_idx_dict[field_name]].squeeze(1)\n",
    "            gmf = (gmf * tmp)\n",
    "        \n",
    "        # MLP 통과\n",
    "        # x.shape: torch.Size([1024, (2 + context_feature 수), 16])\n",
    "        # x.view(-1, self.embed_output_dim).shape: torch.Size([1024, 32])\n",
    "        # self.mlp(x.view(-1, self.embed_output_dim)).shape: ([1024, (2 + context_feature 수)56])\n",
    "        # breakpoint()\n",
    "        # shape '[-1, 176]' is invalid for input of size 163840\n",
    "        x = x.view(-1, self.embed_output_dim)\n",
    "        # breakpoint()\n",
    "        x = self.mlp(x)\n",
    "        # breakpoint()\n",
    "        x = torch.cat([gmf, x], dim=1)\n",
    "        x = self.fc(x).squeeze(1)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
